#!/usr/bin/env python3
"""
speed_benchmark.py - Compare simulator speeds

Tests all three simulator versions:
1. simulator.py (original)
2. simulator_fast.py (10-20x speedup)
3. simulator_ultra.py (30-50x speedup)

Shows speed vs fidelity trade-offs.
"""

import time
import numpy as np
from data_loader import create_default_brain


def benchmark_simulator(simulator_module, brain_data, duration=1000.0, name="Simulator"):
    """Benchmark a simulator module."""
    print(f"\n{'='*70}")
    print(f"Testing: {name}")
    print(f"{'='*70}")

    try:
        # Import and setup
        Simulator = simulator_module.BrainNetworkSimulator
        Config = simulator_module.SimulationConfig

        config = Config(duration=duration, transient=100.0)

        # Run simulation
        start_time = time.time()
        sim = Simulator(brain_data, config)
        results = sim.run_simulation()
        elapsed = time.time() - start_time

        # Extract metrics
        mean_act = results['E'].mean()
        std_act = results['E'].std()
        max_act = results['E'].max()
        min_act = results['E'].min()

        # Report
        print(f"\nâ±ï¸  Time: {elapsed:.2f} seconds")
        print(f"ðŸ“Š Mean activity: {mean_act:.3f}")
        print(f"ðŸ“Š Activity std: {std_act:.3f}")
        print(f"ðŸ“Š Range: [{min_act:.3f}, {max_act:.3f}]")
        print(f"ðŸ“Š Timepoints: {len(results['time'])}")

        return {
            'name': name,
            'time': elapsed,
            'mean': mean_act,
            'std': std_act,
            'max': max_act,
            'min': min_act,
            'num_points': len(results['time'])
        }

    except Exception as e:
        print(f"âŒ Error: {e}")
        return None


def main():
    print("\n")
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 20 + "SIMULATOR SPEED BENCHMARK" + " " * 23 + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()

    # Create brain
    print("Creating 68-region brain model...")
    brain = create_default_brain(68)

    # Test duration
    test_duration = 1000.0  # 1 second simulation

    print(f"\nBenchmark parameters:")
    print(f"  - Brain regions: 68")
    print(f"  - Simulation duration: {test_duration} ms")
    print(f"  - Connections: ~1000")

    results = []

    # Test 1: Original simulator
    print("\n" + "â–ˆ" * 70)
    print("TEST 1: Original Simulator (simulator.py)")
    print("â–ˆ" * 70)

    try:
        import simulator as sim_original
        result = benchmark_simulator(sim_original, brain, test_duration, "Original")
        if result:
            results.append(result)
    except Exception as e:
        print(f"âš ï¸  Could not test original simulator: {e}")

    # Test 2: Fast simulator
    print("\n" + "â–ˆ" * 70)
    print("TEST 2: Fast Simulator (simulator_fast.py)")
    print("â–ˆ" * 70)

    try:
        import simulator_fast as sim_fast
        result = benchmark_simulator(sim_fast, brain, test_duration, "Fast")
        if result:
            results.append(result)
    except Exception as e:
        print(f"âš ï¸  Could not test fast simulator: {e}")

    # Test 3: Ultra-fast simulator
    print("\n" + "â–ˆ" * 70)
    print("TEST 3: Ultra-Fast Simulator (simulator_ultra.py)")
    print("â–ˆ" * 70)

    try:
        import simulator_ultra as sim_ultra
        result = benchmark_simulator(sim_ultra, brain, test_duration, "Ultra-Fast")
        if result:
            results.append(result)
    except Exception as e:
        print(f"âš ï¸  Could not test ultra simulator: {e}")

    # Summary comparison
    if len(results) >= 2:
        print("\n" + "=" * 70)
        print("SUMMARY COMPARISON")
        print("=" * 70)

        print(f"\n{'Simulator':<20} {'Time (s)':<12} {'Speedup':<10} {'Mean Act':<12} {'Std Act':<10}")
        print("-" * 70)

        baseline_time = results[0]['time']

        for r in results:
            speedup = baseline_time / r['time']
            print(f"{r['name']:<20} {r['time']:<12.2f} {speedup:<10.1f}x {r['mean']:<12.3f} {r['std']:<10.3f}")

        # Accuracy comparison
        print("\n" + "=" * 70)
        print("ACCURACY COMPARISON")
        print("=" * 70)

        if len(results) >= 2:
            baseline_mean = results[0]['mean']
            print(f"\n{'Simulator':<20} {'Mean Diff':<15} {'Status'}")
            print("-" * 70)

            for r in results:
                diff = abs(r['mean'] - baseline_mean)
                diff_pct = (diff / baseline_mean) * 100 if baseline_mean > 0 else 0

                if diff_pct < 5:
                    status = "âœ… Identical"
                elif diff_pct < 15:
                    status = "âœ… Very close"
                elif diff_pct < 30:
                    status = "âš ï¸  Similar"
                else:
                    status = "âŒ Different"

                print(f"{r['name']:<20} {diff_pct:<15.1f}% {status}")

        # Recommendations
        print("\n" + "=" * 70)
        print("RECOMMENDATIONS")
        print("=" * 70)

        print("\nðŸ“Œ When to use each simulator:")
        print()
        print("  simulator.py (Original):")
        print("    - Maximum compatibility")
        print("    - Reference implementation")
        print("    - Debugging / validation")
        print()
        print("  simulator_fast.py (Fast):")
        print("    - Production use (10-20x faster)")
        print("    - Full fidelity (dt=0.1ms)")
        print("    - Recommended default âœ…")
        print()
        print("  simulator_ultra.py (Ultra-Fast):")
        print("    - Exploratory analysis (30-50x faster)")
        print("    - Parameter tuning")
        print("    - Slight fidelity loss (dt=0.2ms)")

    print("\n" + "=" * 70)
    print("âœ… Benchmark complete!")
    print("=" * 70)


if __name__ == "__main__":
    main()
